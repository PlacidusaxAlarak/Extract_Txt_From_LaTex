import json
import os
import subprocess
import sys
import time
import urllib.error
import urllib.request
from pathlib import Path

from openai import OpenAI

# --- é…ç½®è·¯å¾„ä¸ŽçŽ¯å¢ƒ ---
TOOLS_DIR = Path(__file__).resolve().parent
ROOT_DIR = TOOLS_DIR.parent

# ç¡®ä¿èƒ½å¯¼å…¥åŒç›®å½•ä¸‹çš„æ¨¡å—
if str(TOOLS_DIR) not in sys.path:
    sys.path.insert(0, str(TOOLS_DIR))

# å°è¯•å¯¼å…¥ MinerU å·¥å…·
try:
    from mineru_vlm_tool import mineru_vlm_pdf_to_markdown  # noqa: E402
except ImportError:
    print("[Error] Could not import 'mineru_vlm_tool'. Please ensure dependencies are installed.")
    sys.exit(1)
try:
    from latex_tool import latex_project_to_text  # noqa: E402
except ImportError:
    print("[Error] Could not import 'latex_tool'. Please ensure dependencies are installed.")
    sys.exit(1)

# ================= æ˜¾å¡ä¸Žè·¯å¾„é…ç½®åŒºåŸŸ =================

# 1. æ¨¡åž‹è·¯å¾„
MODEL_PATH = "/inspire/hdd/project/exploration-topic/public/downloaded_ckpts/Qwen2.5-7B-Instruct"
SERVED_MODEL_NAME = "Qwen2.5-7B-Instruct"

# 2. æ˜¾å¡åˆ†é…ç­–ç•¥
VLLM_GPU_ID = "0"
VLLM_TP_SIZE = 1 
MINERU_GPU_ID = "1"

# 3. æœåŠ¡ç«¯å£
HOST = "127.0.0.1"
PORT = 8000
SERVER_URL = f"http://{HOST}:{PORT}/v1"

# ====================================================

READY_TIMEOUT_SEC = 300
READY_POLL_SEC = 2


def _start_vllm_server() -> subprocess.Popen:
    """å¯åŠ¨ vLLM æœåŠ¡å™¨"""
    print(f"[System] Starting vLLM server on port {PORT}...")
    
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = VLLM_GPU_ID

    cmd = [
        sys.executable, "-m", "vllm.entrypoints.openai.api_server",
        "--model", MODEL_PATH,
        "--served-model-name", SERVED_MODEL_NAME,
        "--tensor-parallel-size", str(VLLM_TP_SIZE),
        "--host", HOST,
        "--port", str(PORT),
        "--trust-remote-code",
        "--max-model-len", "32768", 
        "--gpu-memory-utilization", "0.9",
        "--enable-auto-tool-choice",
        "--tool-call-parser", "hermes",  
    ]

    return subprocess.Popen(
        cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True,
    )


def _wait_for_ready(timeout_sec: int = READY_TIMEOUT_SEC) -> None:
    print("[System] Waiting for vLLM server to be ready...")
    deadline = time.time() + timeout_sec
    url = f"{SERVER_URL}/models"
    while time.time() < deadline:
        try:
            with urllib.request.urlopen(url, timeout=5) as resp:
                if resp.status == 200:
                    print("[System] Server is ready!")
                    return
        except Exception: 
            pass
        time.sleep(READY_POLL_SEC)
    raise TimeoutError(f"vLLM server not ready after {timeout_sec}s.")


def _build_tools_schema() -> list:
    return [
        {
            "type": "function",
            "function": {
                "name": "mineru_vlm_pdf_to_markdown",
                "description": "Used to convert PDF files to Markdown content. Use this when user asks to read, parse, summarize or explain a PDF file.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "pdf_path": {
                            "type": "string", 
                            "description": "The absolute file path of the PDF."
                        },
                        "backend": {"type": "string", "enum": ["vlm-transformers"], "default": "vlm-transformers"},
                    },
                    "required": ["pdf_path"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "latex_project_to_text",
                "description": "Convert a LaTeX project directory or archive to plain text.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "project_path": {
                            "type": "string",
                            "description": "Directory or archive path (zip/tar.*) containing a LaTeX project."
                        },
                        "engine": {
                            "type": "string",
                            "enum": ["pandoc", "latexml", "plastex", "pylatexenc"],
                            "default": "pandoc"
                        },
                        "fallback_mode": {
                            "type": "string",
                            "enum": ["rule_then_llm", "llm_only", "rule_only"],
                            "description": "Fallback strategy when Pandoc fails."
                        }
                    },
                    "required": ["project_path"]
                },
            },
        },
    ]


def _execute_tool_logic(tool_name: str, args: dict) -> str:
    """æ‰§è¡Œ MinerU å·¥å…·çš„å°è£…å‡½æ•° (å«ä¿®å¤é€»è¾‘)"""
    if tool_name == "latex_project_to_text":
        project_path = args.get("project_path") if isinstance(args, dict) else None
        if not project_path:
            return "Error: project_path is required."
        engine = args.get("engine", "pandoc") if isinstance(args, dict) else "pandoc"
        fallback_mode = args.get("fallback_mode") if isinstance(args, dict) else None
        return latex_project_to_text(project_path, engine=engine, fallback_mode=fallback_mode)

    if tool_name != "mineru_vlm_pdf_to_markdown":
        return f"Error: Unknown tool {tool_name}"

    print(f"\n[System] Tool '{tool_name}' triggered.")
    print(f"[System] Args: {args}")
    print(f"[System] Switching context to GPU {MINERU_GPU_ID}...")

    # å°è¯•è®¾ç½®çŽ¯å¢ƒå˜é‡ (æ³¨æ„ï¼šå¦‚æžœåœ¨ import æ—¶ torch å·²åˆå§‹åŒ–ï¼Œæ­¤å¤„å¯èƒ½æ— æ•ˆï¼Œå»ºè®®ä½¿ç”¨å­è¿›ç¨‹éš”ç¦»)
    os.environ["CUDA_VISIBLE_DEVICES"] = MINERU_GPU_ID
    
    # å¼ºåˆ¶è®¾ç½® args
    args["backend"] = "vlm-transformers"

    result = None
    try:
        # å…¼å®¹ StructuredTool (LangChain) å’Œ Runnable ä»¥åŠæ™®é€šå‡½æ•°
        if hasattr(mineru_vlm_pdf_to_markdown, "run"):
            result = mineru_vlm_pdf_to_markdown.run(args)
        elif hasattr(mineru_vlm_pdf_to_markdown, "invoke"):
            result = mineru_vlm_pdf_to_markdown.invoke(args)
        else:
            result = mineru_vlm_pdf_to_markdown(**args)
    except Exception as e:
        return f"Error executing MinerU: {str(e)}"

    # è§£æžç»“æžœ
    markdown = ""
    if isinstance(result, dict):
        markdown = result.get("markdown", "")
    elif isinstance(result, str):
        try:
            parsed = json.loads(result)
            markdown = parsed.get("markdown", result) if isinstance(parsed, dict) else result
        except json.JSONDecodeError:
            markdown = result
    
    if not markdown:
        return "Tool executed successfully but returned empty content."
    
    print(f"[System] Tool execution finished. Content length: {len(markdown)} chars.")
    return markdown


def chat_loop():
    """äº¤äº’å¼å¯¹è¯ä¸»å¾ªçŽ¯"""
    client = OpenAI(base_url=SERVER_URL, api_key="EMPTY")
    tools = _build_tools_schema()
    
    # åˆå§‹åŒ–åŽ†å²æ¶ˆæ¯
    messages = [
        {
            "role": "system", 
            "content": "You are a helpful AI assistant. You have access to a tool `mineru_vlm_pdf_to_markdown` to read PDF files and a tool `latex_project_to_text` to convert LaTeX projects or archives to plain text. When the user asks you to read/parse a PDF, call the PDF tool. When the user asks to convert a LaTeX project/archive to text, call the LaTeX tool. After the tool returns the content, answer the user's question based on that content."
        }
    ]

    print("\n" + "="*50)
    print(" ðŸš€ Chat Session Started")
    print(" ðŸ’¡ Type your message and press Enter.")
    print(" ðŸ’¡ Example: 'è¯·å¸®æˆ‘è¯»ä¸€ä¸‹ /path/to/paper.pdf'")
    print(" ðŸ’¡ Type 'exit' to quit.")
    print("="*50 + "\n")

    while True:
        try:
            # 1. ç­‰å¾…ç”¨æˆ·è¾“å…¥
            user_input = input("\nUser: ").strip()
            if not user_input:
                continue
            if user_input.lower() in ["exit", "quit"]:
                print("[System] Exiting chat.")
                break

            # 2. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°åŽ†å²
            messages.append({"role": "user", "content": user_input})

            # 3. è°ƒç”¨æ¨¡åž‹ (ç¬¬ä¸€æ¬¡)
            print("[System] Thinking...")
            response = client.chat.completions.create(
                model=SERVED_MODEL_NAME,
                messages=messages,
                tools=tools,
                tool_choice="auto"
            )
            
            msg = response.choices[0].message
            messages.append(msg) # å°†æ¨¡åž‹çš„å›žå¤ï¼ˆå¯èƒ½æ˜¯æ–‡æœ¬ï¼Œå¯èƒ½æ˜¯å·¥å…·è°ƒç”¨ï¼‰åŠ å…¥åŽ†å²

            # 4. æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
            if msg.tool_calls:
                for tool_call in msg.tool_calls:
                    func_name = tool_call.function.name
                    try:
                        func_args = json.loads(tool_call.function.arguments)
                    except json.JSONDecodeError:
                        print(f"[Error] Failed to parse arguments for {func_name}")
                        continue

                    # æ‰§è¡Œå·¥å…·
                    tool_output = _execute_tool_logic(func_name, func_args)

                    # å°†å·¥å…·ç»“æžœåŠ å…¥åŽ†å² (role='tool')
                    messages.append({
                        "role": "tool",
                        "content": tool_output,
                        "tool_call_id": tool_call.id
                    })

                # 5. å·¥å…·æ‰§è¡Œå®ŒåŽï¼Œå†æ¬¡è°ƒç”¨æ¨¡åž‹ä»¥ç”Ÿæˆæœ€ç»ˆå›žç­”
                print("[System] Feeding tool output back to model...")
                final_response = client.chat.completions.create(
                    model=SERVED_MODEL_NAME,
                    messages=messages, # æ­¤æ—¶åŒ…å«ï¼šUser -> Assistant(Call) -> Tool(Result)
                    tools=tools
                )
                final_msg = final_response.choices[0].message
                print(f"\nModel: {final_msg.content}")
                messages.append(final_msg) # ä¿å­˜æœ€ç»ˆå›žç­”
            
            else:
                # å¦‚æžœæ²¡æœ‰è°ƒç”¨å·¥å…·ï¼Œç›´æŽ¥æ‰“å°å›žç­”
                print(f"\nModel: {msg.content}")

        except KeyboardInterrupt:
            print("\n[System] Interrupted by user.")
            break
        except Exception as e:
            print(f"\n[Error] Unexpected error: {e}")


def main() -> None:
    server = _start_vllm_server()
    try:
        _wait_for_ready()
        chat_loop()
    finally:
        print("[System] Shutting down server...")
        if server.poll() is None:
            server.terminate()
            try:
                server.wait(timeout=10)
            except subprocess.TimeoutExpired:
                server.kill()

if __name__ == "__main__":
    main()
